function [r_sum, koef_all, states_all, action_all] = simulate_episode(method)

global epsilon alpha num_steps_all num_koef q pi gamma n_states n_actions ...
    n_actuators env restricted_actuators coord_dvig action state_dvig ...
    state_env s s_prime koef_all_m koef_t reward;
reseting;
savings;
initials;
r_sum = [];
states_all = [];
action_all = [];
koef_all = [];
n_steps = 0;
num_states = 0;
action = 0;
reward = 0;
s_prime = 1;

for i = 1:num_steps_all
    n_steps = n_steps+1;
    
    % take action from policy
    choose_actions;
    
    % take step
    step;
    
    % use one of the methods
    if method == 1
        A = find(q(s_prime, :) == max(q(s_prime, :)));
        a = randi(length(A));
        a_prime = A(a);
        q(s, action) = q(s, action) + alpha * (reward + gamma * q(s_prime, a_prime) - q(s, action));
    end
    
    if method == 2
        [n, a_prime] = max(cumsum(pi(s_prime, :)) > x);
        q(s, action) = q(s, action) + alpha * (reward + gamma * q(s_prime, a_prime) - q(s, action));
    end
    
    if method == 3
        q(s, action) = q(s, action) + alpha * (reward + gamma * dot(pi(s_prime, :), q(s_prime, :)) - q(s, action));
    end

    % update policy
    B = find(q(s, :) == max(q(s, :)));
    b = randi(length(B));
    best_a = B(b);
    for i = 1:n_actions
        if i == best_a
            pi(s, i) = 1 - (n_actions - 1) * (epsilon / n_actions);
        else
            pi(s, i) = epsilon / n_actions;
        end
    end
    
    %decay gamma close to the end of the episode
    if n_steps > 90000
        gamma = gamma * 0.99995;
    end
    
    % get information
    states_all = [states_all; s_prime];
    r_sum = [r_sum; reward];
    koef_all = [koef_all; koef_t];
    action_all = [action_all; action]; 
    
end  
